  In this post, I hope to explore different forms of “testing in production”, when each form of testing is the most beneficial as well as how to test services in production in a safe way.  
    However, before I proceed any further, I need to make it explicitly clear that the content of this post only ever applies to the testing of services whose deployment is under the control of engineers developing it. By absolutely no means whatsoever am I suggesting that this form of testing is applicable, let alone advisable, for testing other types of software, such as mobile applications or safety critical systems or embedded software.  
    It’s also perhaps important to state upfront that none of the forms of testing described here is easy and often requires a fundamental change in the way systems are designed, developed and tested. And despite the title, I do not believe that any form of testing in production is entirely risk-free; only that it greatly helps minimize the risk profile of the service further down the road, making the investment justifiable.  
    A staging cluster or a staging environment can mean different things to different people. At many companies, “deploying” to staging and testing in staging is an integral precursor to the final rollout.  
    Most organizations I know of treat staging as a miniature replica of the production environment. In such cases, keeping the staging environment as “in-sync” as possible with production becomes a requirement. This usually involves running a “different” instance of stateful systems like databases and routinely syncing production data to staging modulo certain sensitive bits of user data that might contain any personally identifiable information (PII) to comply with GDPR, PCI, HIPAA and so forth.  
    The problem with this approach — at least in my experience — is that the “difference” isn’t just curtailed to using a different instance of the database populated with recent production data. Often the difference also extends to:  
    — the size of the staging cluster (if it even can be called a “cluster” — sometimes it’s single machine masquerading as a cluster)— the fact that staging is usually a much smaller cluster also means that configuration options for pretty much every service is going to be different. This is applicable to the configurations of load balancers, databases and queues (like the number of open file descriptors, the number of open connections to the database, the size of the thread pool, the number of Kafka partitions and so forth). If configuration happens to be stored in either a database or a key value store like Zookeeper or Consul, these auxiliary systems need to be stood up in the staging environment as well.— the number of in-flight connections a stateless service is handling or the way a proxy is reusing TCP connections (if it’s doing it at all)— the lack of monitoring for the staging environment. And even if monitoring exists, several staging monitoring signals could end up being completely inaccurate given that one is monitoring a “different” environment than the production environment. For example, even with monitoring in place for the MySQL query latency or the response time of an endpoint, it becomes hard to detect if new code contains a query that may trigger a full table scan in MySQL, since it’s way faster (sometimes even preferable) to do a full table scan for a small table used in a test database as opposed to against the production database where the query might have a vastly different performance profile.  
    While it’s fair enough to suggest that all of the aforementioned differences aren’t so much arguments against staging environments per se than anti-patterns to be avoided, it also follows that “doing it right” often entails investing a copious amount of engineering effort to attain any semblance of parity between the environments. With production being ever changing and malleable to a wide variety of forces, trying to attain said parity might be akin to being on a hiding to nothing.  
    Moreover, even if the staging environment can be mimicked to be as close to production as possible, there are certain other forms of testing that simply do not lend themselves well to being tested without real traffic. A good example would be soak testing, which is a form of verification of a service’s reliability and stability over long period of time under a realistic level of concurrency and load, used to detect memory leaks, GC pause times, CPU utilization and so forth over a period of time.  
    None of this is meant to suggest that maintaining a staging environment is completely useless (as it would become evident in the section on shadowing). Only that, as often as not, it’s relied upon to a much greater degree than necessarily needed, to the point where at many organizations it remains the only form of testing that happens before a full production rollout.  
    Testing in production has historically carried with it a certain stigma and negative connotations linked to cowboy programming. By means of implicit association, “testing in production” often connotes insufficient or absent unit and integration testing, as well as a certain recklessness or lack of care for the end user experience.  
    When done poorly or haphazardly, “testing in production” does, in fact, very much live up to this reputation. Testing in production is by no means a substitute to pre-production testing nor is it, by any stretch, easy. In fact, I’d argue that being able to successfully and safely test in production requires a significant amount of automation, a firm understanding of the best-practices as well as designing the systems from the ground up to lend themselves well toward this form of testing.  
    In order to be able to craft a holistic and safe process to effectively test services in production, it becomes salient to not treat “testing in production” as a broad umbrella term to refer to a ragbag of tools and techniques. I was guilty of this myself in my previous post where I’d introduced a not very scientific taxonomy of the testing spectrum and had grouped together a miscellany of methodologies and tools under “testing in production”.  
    Since the publication of that post in late December 2017, I’ve had conversations with several people about the content of the post as well as on the broader topic of “testing in production” in general.  
    In light of these discussions as well as certain orthogonal discussions, it became painfully obvious to me that the topic of “testing in production” isn’t something that can be reduced to a few bullet points on an illustration.  
    Instead, I now believe that “testing in production” encompasses an entire gamut of techniques across three distinct phases.  
    This list is by no means comprehensive nor are the boundaries set in stone. However, I feel it does provide a good starting point to begin the discussion on the topic of testing in production.  
    For the most part, discussions about “production” are only ever framed in the context of “deploying” code to production or monitoring or firefighting production when something goes awry.  
    I have myself used terms such as “deploy”, “release”, “ship” and so forth interchangeably until now without giving it much thought. In fact, up until a few months ago, any attempt to make any distinction between these terms would’ve been dismissed by me as immaterial.  
    Upon giving this more thought, I’ve come around to the idea that there does exist a legitimate need to make a distinction between different phases of production.  
    With testing (even in production!) only ever being a best effort verification, the accuracy of testing (or any form of verification for that matter) is contingent on the tests being run in a manner that resembles the way in which the service might be exercised in production in the closest possible way.  
    The tests, in other words, need to be run in an environment that’s a best effort simulation of the production environment.  
    And in my opinion, the best effort simulation of the production environment is … the production environment itself.  
    To run as many tests as possible in the production environment, it’s required that failure of any such test should not affect the end user.  
    Which, in turn, is only feasible if deploying a service in the production environment doesn’t expose users to that service immediately.  
    In this post, I’ve decided to go with the terminology used in the blog post “Deploy != Release” by the folks at Turbine Labs (no affiliation). The post goes on to describe a “deploy” as:  
    Deployments being a zero-risk activity might be music to the ears of a good many folks who bear battle scars from bad deploys (including yours truly). The ability to safely install software in the actual production environment without exposing users to the newly installed software has numerous benefits when it comes to testing.  
    For one thing, it minimizes — if not entirely eliminates — the need to maintain separate “dev”, “test” and “staging” environments which invariably then become dependencies to be kept in-sync with production.  
    It also applies a certain design pressure on engineers to decouple their services in a manner that the failure of a test run in production against a given instance of a service does not lead to a cascading or user-impacting failure of other services. One such design decision would be the design of data model and database schema so that non-idempotent requests (primarily writes):  
    — can be performed against the production database by any service’s test run in production (my preferred approach)— or be safely discarded at the application layer before it hits the persistence layer— or be distinguished or isolated at the persistence layer via some mechanism (such as storing additional metadata)  
    The blog post Deploy != Release goes on to define “release” as:  
    The Google SRE book uses the term “rollout” in the chapter on Release Engineering to describe the release process.  
    A “release” or a “rollout” in this nomenclature refers to what the word “deploy” usually means in common parlance, and terms frequently used to describe different deployment strategies such as “blue-green deploys” or “canary deploys”, again, refer to the release of new software.  
    Furthermore, a bad release of software is what then becomes the cause of a partial or fully blown outage. This also is the phase when a rollback or a roll forward is performed if the newly released version of the service proves to be unstable.  
    The release process usually works best when it’s automated and incremental. Likewise, rollback or rollforward of the service works best when automated by correlating the error rates or request rates with a baseline.  
    If the release of the service went without a hitch and the newly released service is serving production traffic without any immediate issues, we can consider the release to be successful. What follows a successful release is what I call the “post-release” phase.  
    Any sufficiently complex system is going to always exist in a certain degree of degradation. These pathologies aren’t reasons to warrant a rollback or a roll forward; instead they need to be observed (for a variety of business and Operational purposes) and debugged when required. As such, “testing” in the post-release phase resembles not so much traditional testing than debugging or gathering analytics.  
    In fact, I’d argue that every component of the entire system ought to be built in-keeping with the reality that no large system is ever fully healthy, and that failure is to be acknowledged and embraced at the time of designing, writing, testing, deploying and monitoring the software.  
    With the three phases of “production” defined, let’s explore the different testing mechanisms available at each of the different phases. Not everyone enjoys the luxury of working on greenfield projects or rewriting everything from scratch. I’ve taken care to make it explicit in this post as to when a certain technique is best suited to a greenfield development environment, and in the absence of it, what teams can do to still reap most of the benefits of the proposed techniques without requiring to make significant changes to what’s already working.  
    The bulk of this post will focus on testing in production during the deploy phase, since I believe that is what is discussed the least in the industry.  
    Assuming we can divorce the “deploy” phase from the “release” phase as outlined above, let’s explore some of the forms of testing that can be performed once the code is “deployed” to production.  
    Traditionally, integration testing is performed by a CI server in an isolated “testing” environment over every git branch. A copy of the entire service topology (including databases, queues, caches, proxies and so forth) is spun up for the test suites of all the services to be run against each other.  
    I believe this to be not terribly effective for several reasons. First of all, as with staging, it’s impossible for the test environment being spun up to be identical to the real production environment, even if the tests are run in the same Docker container that will be deployed to production. This is especially true when the only thing that’s running in the test environment are the tests in question.  
    Furthermore, irrespective of whether the test being run is spawned as a Docker container or a POSIX process, the test is most likely spawning one (or whatever level of parallelism is built into the test harness) connection to an upstream service or a database or a cache, which is rarely ever the case when the service is in production where it might be handling multiple concurrent connections at the same time, oftentimes reusing inactive TCP connections (called HTTP connection reuse).  
    Also somewhat problematic is that fact that most tests spawn a fresh database table or a cache keyspace often on the same host on which the test is running during each test run (isolating the tests from network failures). This form of testing can only, at best, prove that the system works correctly for a very specific request (at a previous job we called this the test snowflake), and rarely, if ever, is effective in simulating the hard, well-understood failure modes, let alone the multiple modes of partial failure. There is sufficient research to corroborate that distributed systems often times exhibit emergent behavior, which cannot be predicted through analysis at any level simpler than that of the system as a whole.  
    None of this is to suggest that integration testing on the whole is useless; only that performing integration tests in a completely isolated and artificial environment is, by and large, pointless. Integration testing is still pretty important in order to verify that a new version of a service does not:  
    — break the contract it exposes to its upstreams or downstreams— affect the SLO of any of the upstreams or downstreams in an adverse way  
    The first can be achieved to an extent with contract testing. By dint of only enforcing that the interfaces between services are adhered to, contract testing proves to be an effective technique to develop and test individual services in the pre-production phase without requiring to spin up the entire service topology. Consumer driven contract testing frameworks like Pact currently only support RESTful JSON RPC communication between services, though I believe support for asynchronous communication over web sockets, serverless applications and message queues is being worked on. While it might be possible that in the future protocols like gRPC and GraphQL might be supported, it doesn’t currently appear to be the case.  
    However, before releasing a new version, one might want to more than just verify if interfaces are adhered to. For instance, one might want to verify that the duration of the RPC call between two services falls within an acceptable threshold when the interface between them changes. Additionally, one might also want to make sure that the cache hit ratio remains consistent when, say, an additional query parameter is added to an incoming request.  
    Integration testing, as it turns out, isn’t optional, and in my opinion, the goal of integration testing is to ensure that the change being tested does not trigger any of the hard, well-understood failure modes of the system (typically the ones that have alerts in place for) at the time of testing.  
    Which begs the question — how to safely perform integration testing in production?  
    Let’s work with the following example to this end. This is an architecture I worked with a couple of years ago where we had multiple web and mobile clients connect to a webserver (service C) backed by MySQL (service D)and fronted by a memcache cluster (service B) .  
    Though this is a fairly traditional stack (and not particularly microservicy), I feel the mix of stateful and stateless services in this architecture makes it a good candidate to be used as an example in this post.  
    Decoupling a release from a deploy means that we can safely deploy a new instance of a service into the production environment.  
    Modern service discovery tools allow a service with a same name to have multiple tags (or labels) which can be used to identify a released version and a deployed version of a service with the same name so that downstreams can then try to only connect to the released versions of the service.  
    Let us assume in this case we’re deploying a new version of service C into production.  
    To test if the deployed version functions correctly, we need to be able to exercise it and confirm that none of the contracts it exposes are broken. The mainspring of loosely coupled services is to allow teams to develop, deploy and scale independently, and in the same spirit, it should also be possible to test independently, something I feel (paradoxically) applies to integration testing as well.  
    Google has a blog post on testing titled Just Say No to More End-to-End Tests, where they describe an integration test as:  
    It follows that integration testing in production should adhere to the same philosophy where it should be sufficient — and perhaps beneficial — to only test small groups of units together. If properly designed, all the upstream dependencies should be sufficiently decoupled from service under test, such that a poorly formatted request from service A should not result in a cascading failure in the architecture.  
    In the case of the example we’re working with, this might mean testing the deployed version of the service C and its interaction with MySQL as shown in the illustration below.  
    While testing reads should be mostly straightforward (unless the read traffic of the service under test ends up populating a cache leading to cache poisoning of data used by released services), testing the deployed code’s interaction with MySQL becomes more tricky when it involves non-idempotent requests that might result in modification to the data.  
    My personal preference is to perform integration testing against the production database. In the past I’ve maintained a whitelist of clients that were allowed to issue requests to the service under test. Certain teams maintain a dedicated set of accounts or users for tests being run against a production system, so that any concomitant change to the data is isolated to a small sample set.  
    If however, it is absolutely required that production data not be modified at any cost during a test run, then writes/updates need to either be:  
    — discarded at the application layer C or be written to a different table/collection in the database— a new entry in the database marked as one created by a test run  
    In the second case, if test writes need to be distinguished at the database layer, then it mandates that the schema for the database be designed upfront to support this form of testing (by having an additional field, for instance).  
    In the first case, discarding writes at the application layer can happen if the application is able to detect that the request is to not to be acted upon — this could be done by inspecting the IP address of client issuing the test request or by the user ID of the incoming request or by checking for the presence of a header in the request the client is expected to set while in the test mode.  
    If what I’m suggesting sounds like mocking or stubbing but at a service level, then that’s not too wide off the mark. And this approach isn’t without its fair share of problems. As Facebook’s whitepaper on Kraken goes on to state:  
    While greenfield projects can be designed in a manner such that such side-effects are minimized or warded off or possibly even eliminated entirely, retrofitting such stubbing into a pre-existing infrastructure can prove to be more trouble than its worth.  
    Service mesh architectures can help to an extent with this form of stubbing. In a service mesh architecture, services are unaware of the network topology and listen on localhost for connections. All communication between services happens through a sidecar proxy. The same architecture described above would look like the following when there’s a proxy added to every hop.  
    If we’re testing service B, the egress proxy of service B can be configured to inject a custom header X-ServiceB-Test for every test request, and the ingress proxy of the upstream C can:  
    — detect the header and send back a canned response to service B— signal to service C that the request is a test request  
    Performing integration testing in this manner also ensures that service B’s test is run against its upstreams while the upstreams are serving normal production traffic, which is likely to be a closer simulation of how the service B will behave when released to production.  
    It’d also be great if every service in the architecture also supported making real API calls but in a test or dummy mode, paving the way for downstreams to be tested against the latest contract exposed by the service but without incurring any modification to real data. This is tantamount to contract testing but at the network level.  
    I find shadowing (also known as a dark launch according to a blog post from Google or in istio parlance called mirroring) to be more beneficial than integration testing in many cases.  
    As the Principles of Chaos Engineering puts it:  
    Shadowing is the technique by which production traffic to any given service is captured and replayed against the newly deployed version of the service. This can happen either in real time where incoming production traffic is bifurcated and routed to both the released and deployed version, or it could happen asynchronously when a copy of the previously captured production traffic is replayed against the deployed service.  
    When I worked at imgix (a startup with 7 engineers, only 4 of whom were systems engineers), dark traffic testing was extensively used to test changes to our image rendering infrastructure. We captured a certain percentage of all incoming requests and sent it to a Kafka cluster by shipping the HAProxy access logs to a heka pipeline which then fed the parsed request stream into Kafka. Prior to the release of any new version of our image processing software, it was tested against the captured dark traffic to ensure it correctly handled the requests. However, our image rendering stack was also largely a stateless service which lent itself especially well to this form of testing.  
    Certain companies prefer not to capture a percentage of traffic but to subject their new software to an entire copy of the incoming production traffic. Facebook’s McRouter (a memcached proxy) supports this form of shadowing of memcache traffic.  
    The downside to shadowing the entire traffic against a deployed service in the production environment is that if such shadowing is done at the time of peak traffic, one might end up requiring 2X the capacity to perform such testing.  
    Proxies like Envoy support real time shadowing of traffic to a different cluster in a fire-and-forget manner. As per the docs:  
    Oftentimes however, it might not be possible or practical to maintain a replica test cluster that’s kept “in-sync” with production (for the same reasons why trying to maintain an in-sync “staging” cluster is problematic). Shadowing, when used to test a newly deployed service with a tangle of dependencies, can end up triggering unintended state changes to the test service’s upstreams. Shadowing a day’s worth of user sign-ups against a deployed version of the service that writes to the production database can lead to the error-rate shooting up to 100% due to to the shadow traffic being perceived as duplicate sign-ups and being rejected.  
    In my experience, shadowing works best for testing idempotent requests or testing stateless services with any stateful backend being stubbed out. Such form of testing then becomes more about testing load, impact and configuration. In such cases, testing of the service’s interaction with the stateful backend for non-idempotent requests can be done with the help of integration testing or in a staging environment.  
    I first heard the word “tap-compare” during a discussion with Matt Knox at the Testing in Production meetup this January. The only reference to the term I’ve been able to find has been in Twitter’s blog post on the topic of productionizing a high SLA service.  
    A separate blog post from Twitter describes tap compare as:  
    The difference between tap compare and shadowing appears to be that in the former case, the response returned by the released version is compared with the response returned by the deployed version, whereas in the latter, the request is mirrored to the deployed version in a fire-and-forget manner.  
    Another tool in this space is GitHub’s scientist. Scientist was built to test Ruby code but has since been ported over to a number of different languages. While useful for certain types of verification, scientist isn’t without it’s share of unsolved problems. According to a GitHub engineer on a Slack I’m a member of:  
    Diffy is a Scala based tool open sourced by Twitter in 2015. The blog post titled Testing without Writing Tests is probably the best resource to understand how tap compare works in practice.  
    Tap compare is a great technique to reach for when we’re testing if two versions return similar results. According to Mark McBride:  
    Tap compare is less useful for testing user engagement or testing if two versions of the service behaves similarly under peak load. As with shadowing, side-effects still remain an unsolved problem, especially if both the deployed version and the version in production are writing to the same database, and as with integration testing, limiting the tap compare tests to use a subset of accounts is one way to get around this problem.  
    For those unfamiliar with what load testing is, this article should be a great starting point. There’s no dearth of open source load testing tools and frameworks, the most popular ones being Apache Bench, Gatling, wrk2, Erlang based Tsung, Siege, Scala based Twitter’s Iago (which scrubs the logs of an HTTP server or a proxy or a network sniffer and replays it against a test instance). I’ve also heard from an acquaintance that mzbench really is the best in class for generating load, and supports a wide variety of protocols such as MySQL, Postgres, Cassandra, MongoDB, TCP etc. Netflix’s NDBench is another open source tool for load testing data stores and comes with support for most of the usual suspects.  
    Twitter’s official blog on Iago sheds more light on the yardsticks of a good load generator:  
    Another form of load testing takes the form of stress testing by means of traffic shifting. An Uber SRE first mentioned this form of testing to me where all of the production traffic was directed to a cluster smaller than the one provisioned for the service, and if things didn’t look good, traffic was shifted back to a larger cluster. This technique is also used by Facebook as per one of their official blog posts:  
    Per an ex-LinkedIn engineer on a Slack I’m a part of:  
    Indeed, a bit of Googling unearthed an entire white paper and a blog post on this topic from LinkedIn:  
    Facebook takes load testing using live traffic to a completely different level with a system called Kraken and the whitepaper on Kraken is well worth a read. This is primarily achieved with the help of traffic shifting where Proxygen’s (Facebook’s load balancer) configuration is reloaded with new values (read off a distributed configuration store) for the edge and cluster weights, which determines how much live traffic will be routed to each cluster and region respectively inside a given POP (point of presence).  
    The monitoring system (Gorilla) reports metrics for various services (as detailed in the table above) and decisions on whether to further continue routing traffic per the weights or whether to reduce or completely abort routing traffic to a specific cluster are made based on the monitoring data reported and the threshold for these values.  
    The new wave of open source infrastructure tooling has made it not just possible but also relatively easy to capture all infrastructural changes as code. It’s also possible to varying degrees to test these changes, even if most pre-production tests for infrastructure as code only assert if the specifications and the syntax is correct.  
    However, not testing new configuration before the release of code has been the cause of a significant number of outages.  
    In order to test configuration changes holistically, it becomes important to draw the distinction between different forms of configuration. Fred Hebert, in a conversation with me, came up with the following quadrant:  
    Again, this is by no means comprehensive, but having this demarcation unlocks the ability to decide how best to test each configuration and at what phase. Build time configuration makes sense if truly reproducible builds are possible. Not all configuration is static and on modern platforms dynamic configuration changes are inevitable (even when dealing with “immutable infrastructure”).  
    Testing configuration changes with the same rigor as code changes is something I’ve rarely seen done. Techniques like integration testing, shadowing peak production traffic and blue-green deploys for configuration changes can greatly minimize the risk of rolling out new configurations.  
    An article from Facebook from a couple of years ago had some good insights into risk management for rolling out configuration changes:  
    Once a service has been tested after it’s deployed, it needs to be released.  
    It’s important here to stress that in this phase, a rollback happens only when there are hard failure modes, such as:  
    — a service crash looping— an upstream timing out a significant number of connections leading to a spike in error rate— a wrong config change such as a missing secret in an environment variable taking down the service (as an aside, environment variables are best avoided, but that’s a different discussion for a different blog post)  
    Ideally, if testing in the deploy phase was thorough, any surprises in the release phase would ideally be minimal to non-existent. However, there still exist a set of best practices to safely release new code.  
    Canarying refers to a partial release of a service to production. A subset of production now consists of the canaries which are then sent a small percentage of actual production traffic after they pass a basic health check. The canary population is monitored as it serves traffic, the metrics being monitored for are compared with the non-canary metrics (the baseline), and if the canary metrics aren’t within the acceptable threshold, a rollback ensues.  
    As to what traffic is directed at the canary is determined by a variety of factors. At several companies, the canaries first only get internal user traffic (also known as dogfooding). If things look good, a small percentage of production traffic is directed at the canary followed by a full rollout. Rollback of a bad canary is best automated and tools like Spinnaker come with built-in support for automated canary analysis and rollback.  
    Canaries aren’t without their fair share of problems and this article provides a good rundown of the trouble with canaries.  
    Monitoring is a must-have at every phase of the production rollout, but is especially important during the release phase. “Monitoring” is best suited to report the overall health of systems. Aiming to “monitor everything” can prove to be an anti-pattern. For monitoring to be effective, it becomes salient to be able to identify a small set of hard failure modes of a system or a core set of metrics. Examples of such failure modes are:  
    — an increase in the error rate— a decrease in the overall request rate to the service as a whole or a specific endpoint or worse, a complete outage— increase in latency  
    Any of these hard failure modes warrant an immediate rollback or rollforward of the newly released software. The one important thing to bear in mind is that monitoring at this stage doesn’t have to be elaborate and comprehensive. Some believe that the ideal number of signals to be “monitored” is anywhere between 3–5, and certainly no more than 7–10. As Facebook’s white paper on Kraken suggests:  
    The set of system and application metrics to monitor during a release is best decided during system design time.  
    I put exception tracking under the release phase though to be fair, I think it has equal utility during the deploy phase and during the post-release phase as well. Exception trackers often don’t get the same scrutiny or hype as some of the other Observability tooling, but in my experience I’ve found exception trackers enormously useful.  
    Open source tools like Sentry furnish rich detail about the incoming requests, stack traces and local variables, all of which makes debugging a lot easier than plainly looking at logs. Exception tracking can also help with the triaging of issues that need fixed but don’t warrant a full rollback (such as an edge case triggering an exception).  
    Traffic shaping or traffic shifting really isn’t so much a standalone form of testing than something to assist with canarying and the gradual release of the new code. In essence, traffic shifting is achieved by updating the configuration of the load balancer to gradually route more traffic to the newly released version.  
    Traffic shifting is also a useful technique to assist with the gradual rollout of new software separate from a normal deploy. At my previous company imgix, in June 2016 a completely new rearchitecture of our infrastructure required to be rolled out. After first testing the new infrastructure with a certain percent of dark traffic, we embarked on a production rollout by routing around 1% of production traffic to the new stack initially. This was followed by gradually scaling up traffic to the new stack (while fixing certain issues that cropped up along the way) over the course of several weeks until 100% of our production traffic was being served by the new stack.  
    The popularity of the service mesh architecture has led to a renewed interest in proxies, with proxies both old (nginx, HAProxy) and new (Envoy, Conduit) releasing support for new functionality to pip each other at the post. A future with automated traffic shifting from 0 to 100% during a production release doesn’t seem very far-fetched to me.  
    Post-release testing takes the form of verification that happens once we’ve satisfactorily released code. At this stage, we’re confident that the code is largely correct and was successfully released to production and is handling production traffic as expected. The code deployed is in real use directly or indirectly, serving real customers or doing some form of work that has some meaningful impact for the business.  
    The goal of any verification at this stage largely boils down to validating that the system works as intended given the different workloads and traffic patterns it might be subject to. This is best achieved by capturing a paper trail of what’s happening in production, and use that for both debugging purposes as well as general understandability of our system.  
    The oldest blog post I can find about a company successfully using feature flags dates back to almost a decade ago. There’s even a website featureflags.io to serve as a comprehensive guide to all things feature flagging.  
    Once new code is deployed to production behind a feature flag, it can then be tested for correctness and performance as and when required in the production environment. Feature flagging is also one of the more accepted forms of testing in production, and as such is well-known and has been extensively written about. What’s perhaps less known is that feature flagging can be extended to testing database migrations or desktop software as well.  
    What’s probably not written about much are the best practices for developing and shipping feature flags. Feature flag explosion has been a problem at a previous company I worked at, and not being disciplined about pruning feature flags not in use after a certain period of time led us to go down the path of a massive clean-up of months (and sometimes years) old feature flags.  
    A/B testing often falls under the experimental analysis and isn’t necessarily seen as a form of testing in production. As such it’s not only widely (sometimes controversially) used but also extensively researched and written about (including what makes good metrics for online experiments). What’s possibly less common is A/B testing being used for testing different hardware or VM configurations, but these are often referred to as “tuning” (for example, JVM tuning) as opposed to being seen as an A/B test (even if in principle, tuning can very much be seen as a form of A/B test to be undertaken with the same level of rigor when it comes to measurement).  
    Known as the “three pillars of Observability”, logs, metrics and distributed traces are something I’ve written about extensively in the past.  
    Profiling applications in production is sometimes required to diagnose performance related problems. Depending on the language and runtime support, profiling can be as easy as adding a single line of code to an application (import _ "net/http/pprof " in the case of Go) or might require significant instrumentation to applications or by means of blackbox inspection of a running process and inspecting the output with a tool like flamegraphs.  
    A lot of people perceive teeing as similar to shadowing, since in both cases production traffic is replayed against a non-production cluster or process. The difference, the way I see it, comes down to the fact that replaying production traffic for testing purposes is slightly different from replaying traffic for debugging purposes.  
    Etsy blogged about using teeing as a technique for verification (this use case seems similar to shadowing)  
    However, in some cases, being able to tee production traffic against an off-line system is required for debugging purposes. In such cases, the offline system might be modified to emit more diagnostics information or be compiled differently (with the thread sanitizer, for instance) to help with troubleshooting. In such cases, teeing becomes more of a debugging tool than a verification tool.  
    When I worked at imgix, such forms of debugging was fairly infrequent but not unheard of, especially when it came to debugging issues with our latency sensitive applications. For instance, here’s a post mortem of one such incident from 2015 (reproduced here with the permission of my incredible ex-coworker Jeremy who debugged this):  
    Engineers working on latency sensitive applications need to be comfortable degugging using captured live traffic, since these are often bugs that cannot be reproduced by a unit test or even surfaced by any Observability tool (especially when logging is latency-prohibitive).  
    Chaos Engineering, first popularized by Netflix’s Chaos Monkey, is now a discipline in its own right. The term “chaos engineering” might be new, but fault injection testing has been around for a long time.  
    Chaos testing refers to techniques such as killing random nodes to see if the system copes or injecting error conditions to verify their proper handling or forcing a misbehaving network to see how the service reacts.  
    The vast majority of organizations lack the Operational sophistication to be able to embark on chaos testing. It’s important to state that the injection of faults in a system is best undertaken once a baseline of resilience is established. This whitepaper by Gremlin (no affiliation) should provide a good primer on chaos testing as well as guidelines on how to get started.  
    The goal of testing in production isn’t to completely eliminate all manner of system failures. To quote John Allspaw:  
    Testing in production might seem pretty daunting at first, way above the pay grade of most engineering organizations. While it’s not easy or entirely risk-free, undertaken meticulously, it can greatly help build confidence in the reliability of the sort of complex distributed systems that are becoming increasingly more ubiquitous in this day and age.  
    By clapping more or less, you can signal to us which stories really stand out.  
    @copyconstruct on Twitter. views expressed on this blog are solely mine and NOT those of my employer.  
    
  URL : https://medium.com/@copyconstruct/testing-in-production-the-safe-way-18ca102d0ef1?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website