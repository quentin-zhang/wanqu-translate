谢谢你,一如既往,弗雷德·赫伯特和Sargun Dhillon阅读这篇文章的草稿和提供一些宝贵的建议。 
 在她速度主旨,玛博科维奇框突出健康检查的重要性,自动化数据库故障转移。特别是,她强调如何监控端到端查询时间是一个更好的方法比简单的ping确定数据库的健康。 
 这导致了一个讨论和我的一个朋友建议健康检查必须尽可能简单,交通是一个更好的生活标准为理解健康的一个过程。 
 通常,讨论围绕健康检查主的实现光谱的两个选择,要么肢体-简单的ping /信号或全面的端到端测试。在这篇文章中,我的目标是强调问题背后使用上述形式的健康检查对于某些类型的负载平衡决策以及需要更细粒度的方法测量健康状况的一个过程。 
 健康检查,甚至在许多现代系统中,通常倾向于属于两类——健康检查主机水平和服务水平的健康检查。 
 例如,Kubernetes实现健康检查使用准备和活性探针。准备调查用于确定一个圆荚体可以提供交通。准备调查的失败将导致的豆荚被从端点中删除服务,导致舱没有任何路由流量,直到准备调查成功。活性探针,另一方面,是用来表示如果服务响应或者挂起或陷入僵局。活性调查的失败导致kubelet重新启动单独的容器中。领事,同样,允许多种形式的检查,可以基于脚本检查或基于http的检查指定的URL或基于TTL检查甚至别名检查。 
 最常见的方式实现一个服务水平健康检查是通过定义一个健康检查端点。例如,在gRPC,健康检查本身成为一个RPC调用。gRPC还允许每个服务健康检查以及整体gRPC服务器健康检查。 
 在过去,主机水平健康检查被用作一个信号警报。一个例子就是提醒CPU负载平均(这些天理所当然地认为是一种反模式)。即使不能直接用于报警、健康检查表单仍然其他的基础自动化基础设施决策,如负载平衡和破坏(occassion)电路。服务网格数据飞机像特使,例如,地方领导健康检查信息服务发现数据时确定是否流量路由到一个实例。 
 萍只能传达服务是否上升或下降,而端到端测试代表系统是否可以执行特定的工作单元,工作可以是执行一个数据库查询或执行一个特定的计算。无论什么形式的健康检查可能需要,健康检查的结果都被视为一个严格二进制结果——健康检查“通过”或“失败”。 
 在现代的、动态的和经常“auto-scaled”基础设施,一个单一的过程仅仅是“了”无所谓说过程是无法完成一个给定的工作单元,呈现简单的检查就像ping几乎毫无用处。 
 虽然很容易告诉当服务是完全,这是更难确定服务的健康程度的活着。这是非常可能的“向上”(即一个过程。,通过健康检查)和路由流量只对它无法完成一个给定的工作单元内,服务的p99延迟。 
 无法完成工作通常是一个过程的结果被重载。在高并发服务,“超负荷”巧妙地映射到一致的数量 
 租金要求只能由一个进程提供服务和排队的过度会导致延迟的增加RPC调用(虽然更常见,下游服务只会超时请求和重试配置超时后)。尤其如此,如果健康检查端点配置为盲目地返回一个HTTP 200状态码,而真正的工作服务包括网络I / O或计算。 
 “健康”的过程是一个谱系。我们真正感兴趣的是服务质量,如需要多长时间进程返回一个给定的工作单元的结果和结果的准确性。 
 过程很可能波动之间的不同程度的健康过程中它的生命周期,从完全健康的(如,能够函数预期水平的并发性)近乎不健康(当队列开始填充),它完全翻转到不健康的区域(此时请求体验退化的服务质量)。只有最简单的服务能够建立在假设不存在某种程度的部分失败,部分失败意味着一些特性上和别人下,不仅“一些请求失败和成功”。如果服务架构不能优雅地处理部分失败,那么责任自动落在客户端处理错误管理的复杂性。 
 自适应、自修复基础设施应与现实建立在保持如此大的波动完全是正常的。同样重要的是要记住,这种区别只是重要的负载平衡而言,它毫无意义,例如,为协调器启动流程仅仅因为即将被重载。 
 换句话说,这是完全合理的编排层治疗过程作为一个二进制的健康状态,只有重启进程崩溃或挂。然而,这是极其重要的,负载均衡层(不管它是一个进程外代理像特使或客户端进程内库)作用于更细粒度的信息流程的健康circuit-breaking和甩负荷相应决策。服务不可能完全降低如果不可能确定的健康服务在任何给定的时间准确。 
 在我的经验中,无限的并发性经常被的主要因素导致服务退化或持续的表现。负载平衡(通过扩展,甩负荷)通常可以归结为有效地管理并发和前施加反压力系统可以重载。 
 马特·兰尼phenonemal博客是无界的并发性和反压力在node . js的必要性。整篇文章是值得一读,但最大的外卖(至少对我来说)是需要之间的反馈循环过程及其下游(通常一个负载均衡器,但有时也可能是另一个服务)。 
 速度限制和电路打破基于静态阈值和限制可以被证明是容易出错和脆弱的正确性和可伸缩性的立场。一些负载平衡器(著名的HAProxy)确实提供了大量的统计信息的内部队列长度在每服务器和后端基础上。HAProxy此外,还允许一个代理来检查(定期健康检查的辅助检查独立)使得流程能够提供更准确和动态反馈代理对其健康。引用文件: 
 这种模式的服务动态交流健康其下游构建自适应基础设施是非常重要的。一个例子将是一个建筑前一份工作与我一起工作。 
 我previousl 
 y在imgix,实时图像处理启动。用一个简单的URL API,图像实时获取并转换然后通过CDN服务在世界任何地方。我们的堆栈是相当复杂(如前所述),但简而言之,我们的基础设施组成的一个工作负载平衡和分布层与原点取层,原点缓存层,图像处理层和内容分发层。 
 我们的负载平衡层的核心是一个服务叫做溢洪道,既作为一个反向代理以及代理的请求。溢洪道是一个纯粹的内部服务;在我们运行nginx和HAProxy,所以溢洪道并不是终止TLS或执行任何其他无数的职权范围内的功能,通常一个边缘代理。 
 溢洪道由两部分组成——一个前端(称为溢洪道FE)和一个代理。虽然最初都住在同一个二进制组件,在路上,我们决定将它们分割为单独的二进制文件在同一个主机上部署在一起。这主要是由于这样的事实,这两个组件有不同性能概要,前端几乎完全被CPU绑定。前端的责任是执行一些对每个请求进行预处理,包括飞行前对我们的起源缓存层,确保图像缓存图像转换请求之前在我们的数据中心外包给一个工人。 
 在任何给定的时间,我们有一个固定的(12个左右,如果我没记错的话)的工人将会连接到一个单一溢洪道代理。这些工人负责执行实际的图像变换(裁剪、调整、PDF处理,GIF渲染等等)。工人加工从几百页PDF文件与数百帧gif图像文件。另一个工人的特质是,尽管所有的网络是完全异步的,实际的转换在GPU上本身不是。考虑我们是一个实时服务,无法预测我们的交通模式在任何时候的样子。这需要我们的基础设施能够自适应不同形状的传入流量而不需要任何手工操作员干预。 
 考虑到不同和千变万化的交通模式我们经常看到,它变成了一个梦想的工人能够拒绝接受传入的请求(即使他们很“健康”)如果接受连接意味着工人都可能被重载。每个请求对职工进行一些元数据请求的性质,使工人能够确定是否在服务请求。每个工人维护自己的一组统计数据,目前操作的请求。这些统计数据结合使用的工人请求元数据和其他启发式如套接字缓冲区大小来确定是否优雅接受传入的请求。当一个工人决定不能接受一个请求,它精心制作了一个响应与HAProxy的代理来检查通知其下游(溢洪道)的健康。 
 溢洪道跟踪池中所有的工人的健康。溢洪道首先试图把连续三次请求分派到不同的工人(喜欢的工人可能原始图像在本地文件系统和不超载),如果发生的所有三个工人拒绝接受请求,该请求将在内存中的排队代理。代理维护队列的三种形式——一个后进先出队列,一个FIFO队列和优先队列。如果发生了三个队列满了,代理只会拒绝请求,允许客户端(HAProxy)补偿期后重试。一旦请求队列中的任何一个 
 三个队列,任何自由工人能够流行的请求队列和处理它。有进一步的复杂性如何优先分配给请求,决定在这三个队列(先进先出,后进先出基于优先级的)任何特定的请求必须放置在,但这些都是这篇文章的范围。 
 这种形式的动态反馈回路是必不可少的健康运行的服务。代理队列的大小(在所有的三个队列)是我们密切监控和我们的一个关键普罗米修斯警报时队列大小超过某一阈值(很少发生相当)。 
 超级已从今年早些时候的一个有趣的帖子,阐明他们的方法来实现一个基于服务质量的卸载层。 
 然而,重要的是要记住,如果反压力不是传播回调用链,会有某种程度的排队在某个组件的分布式系统。谷歌一个臭名昭著的文章在2013年发表名为规模的尾巴,这感动在几个原因延迟变化在大系统中扇出(排队是很重要),以及一些简洁的技术(通常涉及冗余请求)来减轻这种可变性。 
 管理流程中并发实时分布式甩负荷的基础,每个组件在系统决策基于本地知识。虽然这有助于满足可伸缩性,无需集中协调,它并不完全排除完全集中率限制的必要性。 
 对那些有兴趣学习更多关于正式的性能建模与排队论,我建议看以下对话: 
 控制回路和反压力已经解决的问题在TCP / IP等协议(拥塞控制算法依赖于负载推理),IP ECN(这是一个明确的机制来确定负载,或接近负载),以太网,比如暂停帧的影响。 
 粗粒度的编排系统健康检查可能就足够了,但被证明是不足以确保服务质量,防止在分布式系统级联故障。负载平衡器需要应用程序级别能见度为了成功而准确地传播给客户的反压力。服务不可能完全降低如果它是不可能确定其健康在任何给定的时间准确。没有及时和充分的反压力,服务可以迅速陷入失败的陷阱。 
 从一个快速起立鼓掌欢呼,鼓掌能表示你吃得多喜欢这个故事。 
 @copyconstruct在Twitter上。观点在这个博客上完全是我的,而不是那些现在或过去的雇主。 
  
   
  URL : https://medium.com/@copyconstruct/health-checks-in-distributed-systems-aa8a0e8c1672?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website