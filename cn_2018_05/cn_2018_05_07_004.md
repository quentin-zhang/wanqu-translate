成员专题报道 
 设计对社会社交工具 
 Facebook内部的老格言:快速行动和打破的东西 
 Facebook内部的老格言:快速行动和打破的东西 
 数据利用影响数百万,选举黑客,报纸的死亡,核武化的宣传,巨魔军队,深化极化和摇摇欲坠的民主本身的未来。似乎我们提供每日清单的反乌托邦的后果有关回到我们共同的过度使用社交媒体。 
 人哭改变,但有一个失踪的对话:尽管许多新兴意识这些平台的可怕的副作用,我们依赖于他们。 
 他们已经成为我们当地新闻频道,我们的应急通信系统,我们的城镇广场,主窗口为我们所爱的人的生活和政府。他们是一个关键的一部分,我们团结在共同的原因和如何与我们的政治。 
 我们如何协调他们与效用的毒性? 
 不应该这样一个痛苦的选择。在过去的一年我一直跟学者、设计师和技术人员研究这些平台的关键缺陷,编目可测试的修复。我的努力都集中在极化的嵌套问题,灭绝人性,和愤怒,这些工具的三个最危险的副产品。 
 下面,我将探讨现在可能被这些公司改变了,不是明天,让这些平台更好的人类。 
 feed可以使我们生气是有原因的。 
 想象一下,你走在街上,你听到的战斗爆发。响亮而咄咄逼人,人们大喊大叫的事。你可能会暂时停止去看发生了什么事——这是你的本性。 
 如果你个人知道的一个人战斗,你可能会立刻选择一个,你甚至可以参与进来。至少,你会注意。 
 这是什么社会媒体对我们定期:它鼓励我们观察冲突和选择方面的话题我们会有一些意见。 
 在其核心,它是一个opinion-serving机器。在社会媒体,并不是所有的观点都是同样。 
 我们的大部分内容提要和时间不再是按照时间顺序排序。决定哪些内容向我们展示而不是基于我们怎么可能去接触它。 
 愤怒等情绪反应强烈的接触指标。最基本的算法类饲料,这种分裂的内容将显示第一,因为它捕获更多的关注比其他类型的内容。 
 这个内容作为触发自己的情绪反应。当我们回应,我们经常把我们的情感世界。 
 这是一个简化模型,如何在社交媒体分享: 
 愤怒的内容,这使得我们可以称之为愤怒瀑布——病毒爆炸的道德判断和厌恶。这些来主导我们的反馈和我们的谈话,并成为一个著名的文化思潮的一部分。 
 纽约大学研究员威廉·j·布雷迪,最近发现一个模式在病毒社交媒体的文章。研究大规模数据集的成千上万的微博,他发现帖子用道德和情感语言获得20%提高为每一个道德和情感的关键字使用。 
 这些tweet包含语言道德指控,和别人的谴责。他们煽动很深的情感反应,更容易看到和共享的人同意他们的观点,提供一个可衡量的促进病毒营销和参与。 
 这是一个隐藏的人们分享分裂生长的基质,无耻的和情感上的在线内容。 
 这并不仅仅适用于我们个人的职位。它可能适用于任何内容我们分享在社交媒体上评论,模因、视频、文章。它孵化一个生态系统的道德义愤 
 利用内容创建者无处不在,包括新闻机构,因为它的工作原理。 
 Facebook、Twitter、YouTube和其他优先考虑这种类型的内容,因为它是我们点击,上空盘旋,并响应。隐藏的途径是观众参与。当试图抓住注意力,我们愤怒、恐惧和厌恶是一个信号的噪声。 
 通过这些工具的优势——在我们的媒体,我们的谈话,和我们的生活——我们看过我们共同的话语把丑陋,分裂,日益两极分化。 
 这是我们可以考虑四个设计更改为提高我们在网上分享的方式: 
 推动人们在正确的方向上与特定的提示之前 
 Molly Crockett耶鲁的克罗克特实验室已经表明,我们的身体无法看到其他人的情绪反应可能会鼓励社会媒体的负面行为。网上我们看不到别人的痛苦,这让我们更愿意是不友善的。 
 耶鲁大学的尼古拉斯·克里斯塔基斯(也)已经表明,现实生活中的社交网络可以受到简单的人工智能来帮助改善组织行为和结果。我们可以考虑加入这样的提示。 
 这些干预措施后,我们可以几个人后让我们更好地推动用户向善和行为。 
 知觉非人化的研究已经表明,我们可能会对人们在数字环境中更多的惩罚性。增加这样的移情反应可能有助于增加我们的数字的感知化身为人类。(这个研究很吸引人:简单的垂直/水平方向上我们的脸在我们的档案照片可以决定别人认为我们值得惩罚。) 
 你的愤怒不太可能听到的另一边。一个更好的思维方式是:越粗暴的推文,不太可能另一边是看到它。这是基于布雷迪的研究表明微博情感/道德语言限制了它们在意识形态的旅游网络。 
 对于那些真正想联系其他观众,这可能会让他们停下来帮助他们重新定义。虽然这可能不是阻止大多数人的炎症后的内容,有些人可能会考虑他们的语言。这可以陷害与基本信息如何使它更易于被其他观众。 
 我们经常做生气的事情,我们以后后悔。时刻暂停,审查和撤销内容标记为伤害的可能性可能会减少分享它在最糟糕的时刻。 
 注意:所有上面的提示也可以使用链接的内容像文章,只要内容索引和标记正确。 
 不同意的人是很难的。不同意的人在公共场合更加困难。有巨大的社会压力在起作用,当我们在公共场合写评论人的帖子:我们展出,争吵一个想法与一群人看我们。给一个私人回复——把它直接默认消息可能会鼓励人们打开侧边栏的对话更少的外部压力。 
 这是所有伟大,但我们如何找到这些帖子呢?那不是麻烦吗?是的,这是。让我们进入杂草。 
 标记和过滤特定内容类型我们更希望看到更少的 
 这比听起来要困难得多。我们需要度量训练算法和人类找到我们不想要的东西。如果我们干预与不正确的一组指标,我们可以很容易在一些非常黑暗的地方。 
 算法是人类智慧的象征,就像任何人类的创造,他们可以继承和放大我们的视角和缺陷。这被称为算法偏差,表现在股票市场失灵,系统性贷款歧视,种族不公正的评价老师的不公平,甚至 
 监狱里的句子。 
 Facebook已经列车的新闻feed算法度量的“有意义”的用户。这种度量的问题是,许多强大的人类反应——比如道德义愤,厌恶和愤怒,广泛被认为是有意义的。 
 要做到这一点,我们需要更好的指标。 
 通过正确测量的内容类型,用户不希望看到更多的,我们可以开始给用户一个菜单的选择,准确地代表他们的喜好,而不仅仅是他们会点击。 
 这些是三种可能的候选人为内容,我们可能更愿意少:愤怒,有毒,令人遗憾。这些都是起点。 
 关键注意:标记内容引发特定的负面反应立即踏板到言论自由的领土。谁说平台应该操纵别人的声音如果他们说事情,某些人找到进攻吗?那不是审查吗?这是一个巨大的,关键问题——我将地址。继续读下去。 
 一旦这些内容类型被确定,我们可以训练监督AI国旗在未来。 
 后悔是最常见的一种情绪后我们觉得花大量的时间在社交媒体。这主要是由于我们点击的内容,尽管自己,我们知道我们不应该。 
 这是基于现在的偏见的概念:自然人类倾向于人们给回报更强的重量接近目前在考虑权衡两个未来的时刻。 
 测量出现偏差是很棘手的,但一开始可能是目前移动设备的简单提示反馈用户消费后一组内容。这将需要一些深思熟虑的设计选择与流动平衡的频率。 
 毒性可以确定通过询问人们给帖子规模从对健康的有毒,有毒被定义为“一个粗鲁,无礼的,或者不合理,可能会让你留下一个讨论。“这是视角的API模型——一个拼图的项目(谷歌的一部分,谁是最早版本的这个工具是相当有缺陷¹²)。做这项工作的关键部分是确保这些术语是由一个多样化的和具有代表性的用户。 
 这是内容,使用道德、情感和other-condemning语言。我们可以通过索引一个道德基础字典定义它最初由Jonathan Haidt和杰西·格雷厄姆和将它与其他字典。这个定义可以扩大和磨练包括other-condemning和hyper-polarizing语言。 
 用户发布负面帖子后,过滤器如何服务内容 
 所有的内容我们分享在社交媒体上已经经过订婚过滤器。Facebook、Twitter和其他内容向上或向下推我们的提要取决于可能抓住注意力(或者你付给他们多少推广)。问题是,某些类型的内容——愤怒,耸人听闻,clickbait等等——自然攻击我们的注意力在不健康的方式。 
 不健康的内容可以按比例down-weighted占其自然病毒营销。当然社交媒体公司已经这样做经常虐待占内容,盗版,垃圾邮件,和成千上万的其他变量。他们也可以做同样的为其他类型的触发的内容。这将会按比例减少的公愤的,有毒的,和令人遗憾的帖子,给他们更多的平等与其他类型的内容。 
 注意:这踏板进一步到言论自由的领域,领域提出过滤泡沫和回音室。我将在下面地址这两个。 
 为用户提供他们自己的控制算法的内容 
 在2016年的总统大选中,当政治愤慨和硫酸流我们的社会媒体的初级产品提要,这似乎是一个坏主意。用户不会只是在自己的misinformation-fil茧 
 使现实?不这只是增加政治孤立?但努力识别虚假新闻、错误信息和宣传获得了蒸汽,这个问题一直支撑我们依赖黑盒算法。透明度缺失的是为什么我们看到我们所看到的社会媒体。 
 的解药是给用户访问编辑过程,确定哪些内容。从最基本的逆时以高度策划提要,这个过程可以打开了。 
 麻省理工学院媒体实验室的一个项目开始这个过程称为遮光黑布开始开发一个开放社会提要聚合器显示仪表盘会是什么样子的一个例子。这允许用户过滤内容,诸如政治、粗鲁、病毒营销。这些指标是一个不错的起点在思考一个开放的饲料可能是什么样子。 
 这可能看起来像一套新工具为探索新的视角和饲料的深处。这些提要可以讨论,重新配置和共享。 
 为用户设计进料控制的过程将打开一个至关重要的关于什么是健康的饮食信息对话框——目前被他们专有的自然的东西。迫使用户决定也可能鼓励用户更多地了解的各种不健康的触发他们被定期服务。 
 这些干预措施是另一种形式的审查?如果我们减少在线的人说我们不同意,这不是压制言论吗? 
 这是一个巨大的问题。 
 很难夸大这些平台有多重要和有影响力的社会在这个时刻。一些最大的、最重要的社会变化,近年来来自激进主义催化通过社交媒体道德义愤共享。许多这样的文化和政治运动将不可能没有他们:# ArabSpring,#茶,# BlackLivesMatter,# MeToo。如果这些声音被压制? 
 但这是关键的问题:社会媒体已经抑制我们的声音。 
 我们目前服务内容不是中立的,无偏见的过程。本质上是不民主,公平分配,或者宪法保护。这些工具已经促进或埋葬的内容我们没有说一个专有的算法。 
 他们不审查政治党派之争。他们审查我们的参与活动,让我们在和连接到这些产品,并为我们的广告。我们一天时间提要成为私有排序机制是这些平台的日子不再是中性的。 
 这些工具更重要成为公共话语,言论自由,民主,更多的问题是这些算法是模糊。 
 这是最终接受的观点——这些工具的重要性,要求所有的控制,我们放弃我们的能力确定谈话的类型作为一个社会。 
 这些都不是完美的解决方案。他们的起点,可以探索更彻底。的一部分,这项工作的目标是推进讨论这些工具基于测试的结果。记住这一点,我将会更新这篇文章的时候有更多的证据和研究。 
 我希望通过仔细挑选、测试和评价这些设计它可能导致我们走向分裂的最终是一个更好的选择,有毒,不健康的数字领域我们共同居住。 
 特别感谢威廉·布雷迪在纽约大学,莫莉Crockett耶鲁,哥伦比亚,卡特里娜飓风芬奇和人文科技中心的见解和研究本文中使用。如果你想保持联系,注册我不更新或者在Twitter上关注我。 
 寻求人类的进步。 
 欢迎来到一个地方,言辞也很重要。在 
 媒介,智能声音和独到的见解采取中心舞台——没有广告。看 
 遵循所有你关心的话题,我们会提供最适合你的故事你的主页和收件箱。探索 
 获得无限制地最好的故事在中期和支持作家当你。5美元/月。升级 
  
   
  URL : https://medium.com/s/story/how-to-fix-what-social-media-has-broken-cb0b2737128?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website