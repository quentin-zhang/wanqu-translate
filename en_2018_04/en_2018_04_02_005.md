   David Kadavy is bestselling author of The Heart to Start & Design for Hackers, & host of Love Your Work Follow @kadavy    
    February 12 2015 – 07:45am  
    There are few things wantrepreneurs (all due respect, I’m a recovering wantrepreneur myself) love to talk about more than running A/B tests.  
    The belief seems to be that if they just keep testing, they will find the answer, and build the business of their dreams.  
    Most of them are wrong. Many of their businesses would be better off if they didn’t run any A/B tests at all.  
    Want to 4x your creative output? Click here for my free toolkit »  
    At the very least, many of these wantrepreneurs would be doing themselves a favor if they would run A/A tests.  
    In an A/A test, you run a test using the exact same options for both “variants” in your test.  
    That’s right, there’s no difference between “A” and “B” in an A/A test. It sounds stupid, until you see the “results.”  
    From June through January, I ran nothing but A/A tests in the emails I sent through MailChimp. 56 different campaigns, comprising more than 750,000 total emails, and I didn’t even test a subject line. (I’ve since switched from MailChimp to ActiveCampaign.)  
    That didn’t stop me from having a six-figure year in my fully-bootstrapped, solopreneur business.  
    During that time, I came across “results” like this.  
    I get a kick out of A/B testing the *exact same* email & finding statistically different performance pic.twitter.com/T6mR8beDnS  
    — David Kadavy (@kadavy) May 30, 2014  
      
    To many wantrepreneurs (my former self included), this looks like “oh wow, you increased opens by 10%!” They may even punch it into Visual Website Optimizer’s significance calculator and see that p=.048. “It’s statistically significant!” they (or I) might exclaim.  
    In fact, when I shared results of A/A tests like this, many people refused to believe me. They’d say things like:  
    “How are the emails different?” (They’re not.)  
    “Were they sent at different times?” (No.)  
    “What did you change?” (Nothing.)  
    IT’S AN A/A TEST! It’s the exact same email, sent at the exact same time, using whatever technology MailChimp uses to send A/B tests.  
    “Ah, but what about MailChimp’s technology?” Well, you could call that into question, but consider this…  
    …to a trained statistician, there is nothing remarkable about these “results.” Given the baseline conversion rate on opens, the sample size simply isn’t large enough to get a reliable result. What’s happening here is just the silly tricks our feeble human minds play on us when we try to measure things.  
    Even if you do have a large enough sample size, you’re bound to get the occasional “false positive” or “false negative.” Meaning that you could make the completely wrong decision based upon false information.  
    Running an A/A test for every email for 8 months really gave me a feel for how misleading A/B test “results” can be. Check out some of the “results” I got from changing nothing at all.  
    A 9% increase in opens!  
      
    A 20% increase in clicks!  
      
    A 51% lower unsubscribe rate!  
      
    Finally, an incredible 300% increase in clicks, all by simply doing absolutely nothing!  
      
    Of course, to an experienced eye, it’s clear that none of these “tests” have a large enough sample size (when taking to account the baseline conversion rate) to be significant.  
    To a wantrepreneur’s eye, however, they’ve just validated (or invalidated) their hypothesis. They may give up their entire vision based upon “results” like these.  
    You can see the “results” of all 56 of these email campaigns compiled in this handy spreadsheet.  
    I spent hours pouring over articles, learning just how to run a reliable, significant A/B test, and I came to this conclusion: It Doesn’t F*cking Matter.  
    To me, it doesn’t matter, and to most budding businesses, it doesn’t matter.  
    Here’s why IDFM.  
    To run a test that asks an important question, that uses a large enough sample size to come to a reliable conclusion, and that can do so amidst a minefield of different ways to be lead astray, takes a lot of resources.  
    You have to design the test, implement the technology, and come up with the various options. If you’re running a lean organization, there are few cases where this is worth the effort.  
    Why create a half-assed “A” and a half-assed “B,” when you could just make a full-assed “A?”  
    As a bootstrapped solopreneur, I realized that every unit of mind energy I used to run an A/B test could have been put towards making one option that was a more lucid interpretation of my vision.  
    In Zero to One, Peter Theil warns against “incrementalism,” or just working to improve what’s already out there. Our world needs entrepreneurs with vision, and if they’re busy second-guessing and testing everything (and often making the incorrect decisions based upon these tests), that’s a sad thing for humanity.  
    Even Eric Ries, one of the forefathers of the Lean Startup movement that has spawned the cult of A/B testing implicitly warns against taking testing too seriously in his book, The Lean Startup:  
    We cannot afford to have our success breed a new pseudoscience around pivots, MVPs, and the like. This was the fate of scientific management, and in the end, I believe, that set back its cause by decades. Science came to stand for the victory of routine work over creative work, mechanization over humanity, and plans over agility. Later movements had to be spawned to correct those deficiencies. –Eric Ries, The Lean Startup  
    Many wantrepreneurs want to use A/B testing as a substitute for having entrepreneurial vision. There’s no doubt that the concepts introduced in The Lean Startup are powerful, but some take it too far, or just misinterpret those ideas.  
    It’s sad, really. It makes you wonder how many great ideas have been (in)”validated” into extinction.  
    I do have a strong vision for my company which is rooted on a tightly-knit point-of-view of How Our World Works. I’m more interested in using my energy to hone that viewpoint (by learning about history, economic and technological trends, etc.), and executing than I am in second-guessing what words I use in a call-to-action button.  
    Unfortunately, the web is packed with misinformation about A/B testing, usually perpetuated either by  
    However, I did come across a few good resources that really explained the complexities of running a reliable test.  
    What I realized was  
    It’s not that I’m bad at math. It’s that I’m not that good at math.  
    Unless you’re someone who is properly trained, and really understands statistics, you should be wary of running tests. Even then, remember that “to someone with a hammer, everything looks like a nail.”  
    At close to 30,000 subscribers, I have a decent-sized email list for a solopreneur. But, what can I test with that? Turns out, not much.  
    Using Evan Miller’s awesome sample size calculator, let’s see what kind of sample sizes I really need.  
    If I want to test the click-through rate on two different emails, my baseline rate would be about 2.2%. That’s about the CTR I can expect if I’m actually trying to get clicks in an email.  
    It turns out, with a list about my size (14,517 per branch = 29,034), I could begin to detect a difference with a change of about .49%, or a 22% increase or decrease in clicks.  
      
    That’s a big difference, and that’s when I would start to “know” I had a winner. The caveat to that being that only 80% of the time (statistical power = 80%) will the difference actually be detected, and 5% of the time (significance level = 5%), I’ll be told there’s a difference when there actually is no difference.  
    To make a solid ROI even more unlikely, a click in an email doesn’t necessarily put money in my pocket. Different types of prospects behave differently.  
    For example, a long email may generate fewer clicks, but the clicks that it does generate will be prospects that are more interested, warmer, and more likely to convert.  
    Using historical data, if I want to find a significant lift in buys from a single email, I’m going to need to make a “B” that converts 70% more customers than option “A.”  
    I’m not Google. With the sample sizes I can put together, I can spend my energy either trying to master the Jedi mind trick, or using best practices to concentrate on making my products and offerings more enticing.  
    There’s no denying the power of the scientific method. When applied correctly, it can be invaluable in guiding an entrepreneur when important questions about a business arise.  
    Here’s a few questions to ask yourself to decide whether you should run an A/B test:  
    Also, keep in mind there are other ways for businesses to “test.” I really like the lets-try-this-and-see-how-it-goes-then-iterate test.  
    The answer is: I don’t really know. In fact, it’s likely that I butchered some of the above terminologies and methodologies (which is the point, really).  
    There really are people out there trying hard to put forth accurate information about A/B testing, but some people just don’t seem to read it. Here are some resources that I found credible and helpful:  
    Running reliable tests that will give you definitive answers is hard.  
    Meanwhile, a 300% increase on a conversion rate of 0% is still 0%. Ship the damn product.  
    Your energy is likely better used elsewhere, and you can start testing when it actually DFM. It’s okay to not want to be a statistics expert.  
    In the meantime, try running a few A/A tests. You’ll be amazed at the “results.”  
    A/A Testing: How I increased conversions 300% by doing absolutely nothing http://t.co/PGN30rUkD5 (New blog post) pic.twitter.com/R27UraX3Fb  
    — David Kadavy (@kadavy) February 12, 2015  
      
    Want to 4x your creative output? Click here for my free toolkit »  
    This post is filed under Entrepreneurship.   
    The Heart to Start will give you the courage to make your art real.  
    Love Your Work (iTunes) will lead you to your calling.   
    That’s amusing and interesting. ? for what it’s worth to people reading when I’ve run AA tests in Optimizely on websites (not emails) I’ve never see hilarious results like those (except at the very very beginning). Once it gets to 100 conversion events ans more it’s almost identical. (At rates higher than a percent or so).   
    The point here is running hundreds of A/A tests: if you run 20 of them 1 of them will show a significant difference at the 5% level. https://xkcd.com/882/ When you run many tests the significance level must become much smaller, e.g. using Bonferroni’s correction. This is just one of the complexities so the article is very right.  
    Hey David I agree that there’s much more to running a/b tests that beginners realize. Most have no clue about needed samples sizes and stuff, and call tests way too early. In that context A/A tests are a good exercise to see that unless your sample size is large enough, the end result is meaningless.  
    That being said – and I love you man – but A/A tests are a fucking waste of time. (Explained over here http://conversionxl.com/aa-testing-waste-time/).  
    So I would rather advise people to “run it once in your life as an exercise to see it with your own eyes”, and then just learn about sample sizes. As I always like to say, statistical significance does not equal validity – does a lot more to declaring A or B as a winner (http://conversionxl.com/statistical-significance-does-not-equal-validity/)  
    Hey Peep, great to see you here!  
    Yeah, I think we’re actually in agreement on the “run it once in your life as an exercise to see it with your own eyes.”  
    No point in everyone running nothing but A/B tests like me. I already did that for you! (and shared the results)  
    I’ve actually found that even among highly trained statisticians very few of them actually understand the meaning of a test’s results. In fact, except in early (VERY early) cases I find all statistically interpreted testing to be largely a waste of time. Any differences found are either misleading or unimportant or your statistician isn’t as smart as he thinks.  
    Hi David, I appreciate you putting this out there, but it saddens me that you simply dismiss the possibility of people developing a better understanding of statistics and statistical tests. There are many ways to strengthen your thesis by looking at the nature by which A and B are intended to vary and what that would actually entail were you going to run that statistical test.  
    More problematic though is you don’t at all discuss the issue of multiple comparisons. Rather than just being an issue of sample size per se, multiple comparisons allow the finding of interesting, extreme “results” by letting you have so many non-interesting, non-extreme “results” that you just ignore as you go along to find the statistic that you will hold up.  
    Finally you have another issue in that the way that you talked about the changes actually relies on a number of implicit assumptions in the data model, especially since some measures are dependent on one another for being able to express even a possibility. For example: forwarded opens are dependent upon #forwards, which means that a comparison of percentages in forward opens would fall into the issue of not comparing like with like. I.e., your data has some structure and so its not quite that you would be comparing apples and oranges, but appple seeds with orange seeds. They may both be seeds, but that doesn’t mean that they can be compared in any way without first considering the hierarchical structure of how the data are arrived at.   
    Nonetheless — it’s good that you’re talking about this, and I appreciate the effort to disabuse people of their overinflated impressions of the usefulness of A/B tests.  
    Cheers, Mike  
    Performing A/B tests using an inadequate sample will yield a predicted result: which usually is: an unpredicted result… (the worst kind being a false positive) but why bother performing A/A tests for a prolonged period of time just to prove that correct sampling is essential? Isn’t it a well known fact?  
    Excellent post. Had a similar experience testing pricing for my micro-SaaS app, turns out it’s just waaay too hard to do a proper A/B test and the guess and check methodology works just fine in 1/20th the time.   
    Really well put.  
    Fun article. Shows that the numbers alone can be misleading. But – why not recommend doing some usability tests or other user research that would give you some ‘why’ alongside the ‘what’?  
    Interesting post.  
    Much of this speaks to the quality of the reports. There will always be differences in conversion rates, but a good report will be clear about which differences are not statistically significant.  
    Optimizely is very good about this. For illustration, I ran an A/A test with 100,000 MDN visitors. Although conversion rates do differ, Optimizely reports <1% confidence in its metrics and mentions that the results are inconslusive.  
    http://optimize.ly/~KfZQEf?token=dd71e4a823cf39c48703#view=2  
    In those kind of AB tests, where you haven’t calculated the efficient sample size in advance, so you know when to stop the test, there might be a p-value fluctuation in an early stage. But in every test checking time-point we need to follow some simple steps/checks: 1) A/B (or A/A) test is designed properly. 2) The tool is doing the right comparison in the background. They usually use 1-tailed tests to make things faster, but this is a mistake when you don’t specify the “winner” in advance. In A/A tests you need to use 2-tail comparison tests as it’s not reasonable to pick a “winner” in advance. 3) In case you find a stat. sign. difference (maybe even after the 2-tail test) you need to check the observed power of the test. In other words, how confident you are the you spotted a real stat. sign. difference.  
    If you manage to check those and still get low p-values with a high power, please just let me know, because I’m really interested in those cases.  
    Thanks.  
    Did you A/B test the impact of font size on your blog? ?  
    Yeah the font size is pretty ridiculous.  
    I’m in a CIA prison. God wrote TempleOS. God is very slow–many many years. I am gaureteed victory.  
    Great post David. Funny, I was just asking @ryanevans if we should A/B test Tend. He said no. It makes sense – energy is better spent elsewhere.  
    OMG I didn’t see this post till now – you are saying what people either don’t understand and have never experienced, or are too afraid to say. You are just using your old noggin, instead of repeating a silly-when-taken-to-the-extreme mantra of test, test, test.  
    Whoa, it’s Terry! Hey, this guy knows ^  
    Hi David,  
    A great read with very good points, both on the need to understand that incremental improvements are no substitute for vision, and on the truth that experimental design and particularly the statistical part of it is d*mn hard (even for scientists)! The “should you test” part is crucial for anyone to understand, though, for the sake of A/B testing itself, the Evan Miller calculator will not remain the “go to” tool for much longer.  
    It is exactly these kinds of realizations that I had several years ago that inspired me to work on what is now the AGILE A/B testing approach: a new statistical method for A/B testing. It borrows statistical methods from the medical field and brings them to online marketing and UX, hopefully making the statistical part of the issues you outline easier to understand and handle.  
    This announcement post: http://blog.analytics-toolkit.com/2017/improved-roi-ab-testing-agile-statistical-method/ contains a link to the free AGILE white paper. I believe it would be of interest to you if you decide to come back to A/B testing again.  
    Best, Georgi  
       
    Disclosure | Privacy | G+ | Coupons  kadavy.net is powered by WordPress. Hosting by WPEngine (Promo Code). Kadavy, Inc. reserves its rights to this site's content under this Creative Commons license.  
    
  URL : http://kadavy.net/blog/posts/aa-testing/?utm_source=wanqu.co&utm_campaign=Wanqu+Daily&utm_medium=website